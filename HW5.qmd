---
title: "ST558_HW5"
author: Keshav Ramesh
format: pdf
---

\newpage

## Task 1 Conceptual questions

1.  What is the purpose of using cross-validation when fitting a random forest model?
    -   cross validation is to check the performance and generalization of the random forest model. Although they already reduce over fitting, cross validation make a more accurate guess of how the model will perform on unseen data
2.  Describe the bagged tree algorithm.
    -   Bagging is an ensemble technique that builds decision tress on different bootstrapped samples of an original dataset. each tree is created independently and the results are averages creating a more stable and accurate prediction.
3.  What is meant by a general linear model?
    -   A general linear model is a statistical model where the outcome is a linear combination of predictors. This includes, simple linear regression, multiple linear regression.

        Y = X_B + e
4.  When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?
    -   The interaction term allow for the model to check situation where one predictor depends on the value of another., It enables the model to check for more complex relationships between variables.
5.  Why do we split our data into a training and test set?
    -   Data is split into training and test sets to evaluate the model's ability to generalize. The model is trained on a portion of a set then used to see if that training holds up on unseen data. This prevents over fitting, and increases accuracy.

\newpage

## Task 2: Data Prep

### Packages and Data

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(yardstick)
library(glmnet)

heart = as_tibble(read_csv("heart.csv"))
```

### Question 1

```{r}
summary(heart)
```

a.  heart disease is being treated as a quantitative variable, it is a storing values as either 1 or 0.
b.  This does not make sense as we should eb treating the variables as categorical, as a yes or no response. While this does work in theory as a binary 1/0 response, it would be better to treat this as a facotrs with levels of yes or no, or T and F.

### Question 2

```{r}
new_heart = heart %>%
  mutate(HeartDisease_FACTORED = factor(HeartDisease, levels = c(0,1), labels = c("N", "Y"))) %>%
  select(-ST_Slope, -HeartDisease)

head(new_heart)
```

\newpage

## Task 3: EDA

```{r}
new_heart %>%
  ggplot(
    aes(
      x = MaxHR,
      y = Age, 
      colour = HeartDisease_FACTORED
    )
  ) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "Age vs Max Heart Rate by Heart Disease Status",
    x = "Max Heart Rate",
    y = "Age",
    color = "Heart Disease"
  ) +
  theme_minimal()
```

### Question 2

An interaction model would be more accurate. The relationship between max heart rate and age differed depending on whether or not the person has heart disease. The have different slops indicating that max hr depends on heart disease status. This is algined with an interactive model not an additive model.

\newpage

## Task 4: Testing and Training

```{r}
set.seed(101)

heart_split = initial_split(new_heart, prop = 0.8)

train = training(heart_split)
test = testing(heart_split)
```

## Task 5: OLS and LASSO

### Question 1

```{r}
ols_mlr = lm(Age ~ MaxHR + HeartDisease_FACTORED + MaxHR*HeartDisease_FACTORED, data = train)

summary(ols_mlr)
```

### Question 2

```{r}
yardstick::rmse_vec(test$Age, predict(ols_mlr, test))
```

### Question 3

```{r}
LASSO_recipe = recipe(Age ~ MaxHR + HeartDisease_FACTORED, data = train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors()) %>%     
  step_interact(~ MaxHR:starts_with("HeartDisease_FACTORED"))

LASSO_recipe
```

### Question 4

```{r}
set.seed(101)
cv_folds = vfold_cv(train, v = 10)

lasso_model = linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")

lasso_workflow = workflow() %>%
  add_model(lasso_model) %>%
  add_recipe(LASSO_recipe)
lasso_workflow
```

```{r}
LASSO_grid = lasso_workflow %>%
  tune_grid(resamples = cv_folds, grid = grid_regular(penalty(), levels = 200))

LASSO_grid
```

```{r}
LASSO_grid %>%
  collect_metrics() %>%
  filter(.metric == "rmse")
```

```{r}
LASSO_grid %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_line()
```

```{r}
lowest_rmse <- LASSO_grid %>%
  select_best(metric = "rmse")
lowest_rmse
```

```{r}
lasso_workflow %>%
  finalize_workflow(lowest_rmse)
```

```{r}
#fit it to the entire training set to see the model fit
LASSO_final <- lasso_workflow %>%
  finalize_workflow(lowest_rmse) %>%
  fit(train)


tidy(LASSO_final)
```

### Question 5

I would expect the outputs to be generally the same between the OLS and LASSO models.

based on the tidy output of LASSO_final we can see that the LASSO model has the same predictors that the OLS model has. This would indiacte that RMSE would also be very similar.

### Question 6

```{r}
ols_mlr %>%
  predict(test) %>%
  rmse_vec(truth = test$Age)



LASSO_final %>%
  predict(test) %>%
  pull() %>%
  rmse_vec(truth = test$Age)

```

### Question 7

They are both using the same predictors, and their interactions are also the same and simple.

## Task 6: Logistic regression

### Question 1

```{r}
logrec1 <- recipe(HeartDisease_FACTORED ~ MaxHR, data = train) %>%
  step_normalize(MaxHR)

logrec2 <- recipe(HeartDisease_FACTORED ~ Age + Sex + ChestPainType + RestingBP, data = train) %>%
  step_normalize(all_numeric(), -HeartDisease_FACTORED) %>%
  step_dummy(all_nominal_predictors())
```

```{r}
set.seed(101)
cv_repeats <- vfold_cv(train, 10, 5)
```

```{r}
log_spec <- logistic_reg() %>%
  set_engine("glm")
```

```{r}
LR1_wkf <- workflow() |>
add_recipe(logrec1) |>
add_model(log_spec)


LR2_wkf <- workflow() |>
add_recipe(logrec2) |>
add_model(log_spec)
```

```{r}
LR1_fit <- LR1_wkf |>
fit_resamples(cv_repeats, metrics = metric_set(accuracy, mn_log_loss))


LR2_fit <- LR2_wkf |>
fit_resamples(cv_repeats, metrics = metric_set(accuracy, mn_log_loss))
```

```{r}
rbind(LR1_fit |> collect_metrics(),
LR2_fit |> collect_metrics())|>
mutate(Model = c("Model1", "Model1", "Model2", "Model2")) |>
select(Model, everything())

mean(train$HeartDisease_FACTORED == "Y")
```

Model 2 was the better performing model it had the highest accuracy and the lowest loss.

### Question 2

```{r}
final_model <- LR2_wkf %>% 
  fit(data = train)

predictions <- predict(final_model, test, type = "class") %>%
  bind_cols(test)

confusionMatrix(
  data = predictions$.pred_class,
  reference = predictions$HeartDisease_FACTORED
)
```

### Question 3

The model's sensitivity was 0.7553: The model will miss about 24.5% of healthy patients. It will false flag almost 1/4 healthy people as diseased.

The model's specificity was 0.8111: The model correctly identified 81% of actual heart disease cases but will miss 19% of diseased patients.

The model is better at confirming positive cases than it is with ruling out disease.
